{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robomimic Lowdim\n",
    "This notebook documents the usage of Robomimic Lowdim in the context of diffusion policies. Lowdim means that the observations using **few variables** (opposed to e.g. images). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading from config file\n",
    "The Robomimic Lowdim dataset contains data for the following tasks:\n",
    "- can\n",
    "- lift\n",
    "- square\n",
    "- tool_hang\n",
    "- transport\n",
    "\n",
    "All of these tasks are loaded with the same `RobomimicReplayLowdimDataset()` dataloader defined in `./diffusion_policy/diffusion_policy/dataset/robomimic_replay_lowdim_dataset.py`. The config file for tasks needs to have a specific structure that can be investigated in `./diffusion_policy/diffusion_policy/config/task/`:\n",
    "\n",
    "```bash\n",
    "dataset:\n",
    "  _target_: diffusion_policy.dataset.robomimic_replay_lowdim_dataset.RobomimicReplayLowdimDataset\n",
    "  dataset_path: *dataset_path\n",
    "  horizon: ${horizon}\n",
    "  pad_before: ${eval:'${n_obs_steps}-1+${n_latency_steps}'}\n",
    "  pad_after: ${eval:'${n_action_steps}-1'}\n",
    "  obs_keys: *obs_keys\n",
    "  abs_action: *abs_action\n",
    "  use_legacy_normalizer: False\n",
    "  rotation_rep: rotation_6d\n",
    "  seed: 42\n",
    "  val_ratio: 0.02\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates how you can use a configuration file to instantiate a dataloader for a dataset of your choice. Note that this configuration file is usually wrapped in another workspace configuration file. We have created an explicit `/config/test/`-folder with a configuration that does not require this instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2052/1801548272.py:14: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with hydra.initialize(config_path=config_path):\n",
      "Loading hdf5 to ReplayBuffer: 100%|██████████| 200/200 [00:00<00:00, 782.17it/s]\n",
      "Loading hdf5 to ReplayBuffer: 100%|██████████| 200/200 [00:00<00:00, 804.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'diffusion_policy.dataset.robomimic_replay_lowdim_dataset.RobomimicReplayLowdimDataset'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd() / \"diffusion_policy\"))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_policy.diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "\n",
    "# step 1: specify the .config file\n",
    "config_path = \"./diffusion_policy/diffusion_policy/config/test\"\n",
    "\n",
    "with hydra.initialize(config_path=config_path):\n",
    "        cfg = hydra.compose(config_name=\"lift_lowdim\")\n",
    "        OmegaConf.resolve(cfg)\n",
    "        dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "\n",
    "# step 2: instantiate dataset from cfg\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "print(type(dataset))\n",
    "\n",
    "# step 3: instantiate DataLoader for dataset\n",
    "train_dataloader = DataLoader(dataset, **cfg.dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data are then processed using the `train_dataloader`. To understand how the processed data looks like, we process an example batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Keys: ['obs', 'action']\n",
      "Observation Shape: torch.Size([256, 10, 19])\n",
      "Action Shape: torch.Size([256, 10, 7])\n",
      "Example Observation: tensor([ 0.0264,  0.0270,  0.8314,  0.0000,  0.0000,  0.9691,  0.2466, -0.1169,\n",
      "        -0.0422,  0.1804, -0.0905, -0.0152,  1.0118,  0.9972, -0.0072,  0.0740,\n",
      "         0.0019,  0.0208, -0.0208])\n",
      "Example Action: tensor([-0.0000,  0.0000,  0.0000,  0.0038,  0.1482,  0.0145, -1.0000])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    \n",
    "    if isinstance(batch, dict):\n",
    "        print(f\"Batch Keys: {list(batch.keys())}\")\n",
    "    \n",
    "    print(f\"Observation Shape: {batch['obs'].shape}\")\n",
    "    print(f\"Action Shape: {batch['action'].shape}\")\n",
    "\n",
    "    observation_batch = batch['obs']\n",
    "    first_observation_sequence = observation_batch[0]\n",
    "    first_observation = first_observation_sequence[0]\n",
    "\n",
    "    action_batch = batch['action']\n",
    "    first_action_sequence = action_batch[0]\n",
    "    first_action = first_action_sequence[0]\n",
    "\n",
    "    print(f\"Example Observation: {first_observation}\")\n",
    "    print(f\"Example Action: {first_action}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the batch size (as specified in the configuration file) is 256. For each sample in the batch, we have 10 observations and actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Preprocessing\n",
    "Now that we now how a single observation looks like, let's understand in more detail which data preprocessing steps are applied by the DataLoader. We start by reading the dataset from its original format, i.e. hdf5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Original Dataset Information =====\n",
      "Dataset Keys: ['data', 'mask']\n",
      "Data Keys: ['demo_0', 'demo_1', 'demo_10', 'demo_100', 'demo_101']\n",
      "Demo Keys: ['actions', 'dones', 'next_obs', 'obs', 'rewards', 'states']\n",
      "Observations: ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_eef_vel_ang', 'robot0_eef_vel_lin', 'robot0_gripper_qpos', 'robot0_gripper_qvel', 'robot0_joint_pos', 'robot0_joint_pos_cos', 'robot0_joint_pos_sin', 'robot0_joint_vel']\n",
      "Actions: <HDF5 dataset \"actions\": shape (59, 7), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "data_path = \"/home/luca_daniel/tum-adlr-04/diffusion_policy/data/robomimic/datasets/lift/ph/low_dim.hdf5\"\n",
    "\n",
    "with h5py.File(data_path, \"r\") as file:\n",
    "    print(\"===== Original Dataset Information =====\")\n",
    "    print(\"Dataset Keys:\", list(file.keys()))\n",
    "    \n",
    "    # the 'data' key of the dictionary contains the demonstrations, i.e. training data\n",
    "    data = file['data']\n",
    "    print(f\"Data Keys: {list(data.keys())[:5]}\")\n",
    "    example_demo = data['demo_0']\n",
    "    print(f\"Demo Keys: {list(example_demo.keys())}\")\n",
    "\n",
    "    print(f\"Observations: {list(example_demo['obs'].keys())}\")\n",
    "    print(f\"Actions: {example_demo['actions']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of *Diffusion Policy* only use observations and actions from the dataset. This data is preprocessed using the following steps:\n",
    "1. `_data_to_obs()`: Transforms training demonstrations into the desired format\n",
    "    - reshapes action dimension, in case a dual arm is used\n",
    "    - extracts `pos`, `rot`, `gripper` from raw action\n",
    "    - applies `RotationTransformer()` to the rotation to get from axis-angle to 6D-rotation\n",
    "    - wraps this into a dictionary with keys `obs` and `action` \n",
    "2. `get_val_mask()`: Determines which of the episodes should be used for validation (specified using a boolean mask)\n",
    "3. `downsample_mask()`: Downsample the training mask, i.e. do not use all *remaining* episodes for training but only a subset\n",
    "4. `SequenceSampler()`: The Sequence Sampler can return a sequence of observation/action-pairs that can be used for training\n",
    "\n",
    "**Note**: The `SequenceSampler` uses a ReplayBuffer to sample sequences from. Each demo is converted to the desired format using `_data_to_obs()`. Once all demos have been added, they are stored in the ReplayBuffer. Each demo is viewed as one *episode* that has very long trajectories. Therefore we can artificially generate more training iterations by dividing the trajectories into smaller chunks to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowdim Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Initialize class using `RobomimicLowdimWrapper()` as described in `robomimic_lowdim_wrapper.py`\n",
    "+ OpenAI Gym Simulation environment\n",
    "    + `.seed(seed)`  sets seed to create environment\n",
    "    + `.step(action)` performs next action and returns `observation`, `reward`, `done`, `info`\n",
    "    + `.reset()` resets the environment (observation), possible to reset to set seed\n",
    "    + `.get_observation()` returns the current observation\n",
    "+ An observation has the following attributes:\n",
    "    + `object`: \n",
    "    + `pos`: position of the robot\n",
    "    + `qpos`: joint positions of the robot\n",
    "    + `quat`: quaternion of the robot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowdim Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize class using `RobomimicLowdimPolicy()` as described in `robomimic_lowdim_policy.py`\n",
    "+ Inherits from `BaseImagePolicy` class as defined in `base_image_policy.py` which has the following key properties:\n",
    "    + `predict_action(obs_dict)`: Function stub to predict the next action given the observation\n",
    "    + `reset()`: Function stub to reset the policy\n",
    "    + `set_normalizer()`: Function stub to set the policy's normalizer\n",
    "+ Extends `BaseLowdimPolicy`\n",
    "    + For initialization\n",
    "        + `get_robo_mimic_config()`: Creates a config file for robomimic based on algorithm, observation type, task, and dataset type\n",
    "        + `algo_factory()`: Initializes a model for the given algorithm based on config file and available actions\n",
    "    + For training\n",
    "        + `train_on_batch()`: Uses observations and actions. Preprocesses a robomimic batch and calls `model.train()` to train the model\n",
    "        + `get_optimizer()`: optimize policy after based on previous policy\n",
    "    + For inference\n",
    "        + `predict_action()`: Predicts the next action given the observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowdim Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize class using `RobomimicLowdimRunner(**lowdimRunner_cfg, output_dir)` as described `robomimic_lowdim_runner.py`. Example usage is given in `test_robomimic_image_runner.py`.\n",
    "+ Inherits from `BaseImageRunner` class as defined in `base_lowdim_runner.py` which has the following key properties:\n",
    "    + `run()`: Function stub to run the policy\n",
    "    + `save()`: Function stub to save the policy\n",
    "    + `load()`: Function stub to load the policy\n",
    "+ Extends `BaseLowdimRunner`\n",
    "    + For initialization\n",
    "        + initializes configuration attributes and paths\n",
    "        + wraps `RobomimicLowdimWrapper` in a `VideoRecordingWrapper` to generate output videos\n",
    "    + For training\n",
    "        + initializes output directory\n",
    "        + configures path for rendered output videos\n",
    "    + For running\n",
    "        + Locates video data and divides it into chunks\n",
    "        + For each chunk, reset policy and observations\n",
    "        + Then, run the simulator (i.e. obtain `action_dict` and call `.step()` until `done` for all chunks)\n",
    "        + Use `env.render()` to add video paths to the output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
